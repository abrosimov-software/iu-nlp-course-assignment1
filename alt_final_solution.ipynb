{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import collections\n",
        "import re\n",
        "\n",
        "###############################\n",
        "# Utility and Data Preparation\n",
        "###############################\n",
        "\n",
        "def words(text):\n",
        "    \"\"\"Extract words from text, lowercasing all characters.\"\"\"\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def build_all_ngrams(file_path):\n",
        "    \"\"\"\n",
        "    Build n-gram counts from a file.\n",
        "    The file is assumed to have at least 6 columns:\n",
        "      count word1 word2 word3 word4 word5\n",
        "    and each line gives a five-gram.\n",
        "    \"\"\"\n",
        "    unigrams = collections.Counter()\n",
        "    bigrams = collections.Counter()\n",
        "    trigrams = collections.Counter()\n",
        "    fourgrams = collections.Counter()\n",
        "    fivegrams = collections.Counter()\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 6:\n",
        "                continue\n",
        "            count = int(parts[0])\n",
        "            tokens = [w.lower() for w in parts[1:6]]\n",
        "            fivegrams[tuple(tokens)] += count\n",
        "            # Two distinct fourgrams per fivegram\n",
        "            fourgrams[tuple(tokens[0:4])] += count\n",
        "            fourgrams[tuple(tokens[1:5])] += count\n",
        "            # Three trigrams per fivegram\n",
        "            trigrams[tuple(tokens[0:3])] += count\n",
        "            trigrams[tuple(tokens[1:4])] += count\n",
        "            trigrams[tuple(tokens[2:5])] += count\n",
        "            # Four bigrams per fivegram\n",
        "            bigrams[tuple(tokens[0:2])] += count\n",
        "            bigrams[tuple(tokens[1:3])] += count\n",
        "            bigrams[tuple(tokens[2:4])] += count\n",
        "            bigrams[tuple(tokens[3:5])] += count\n",
        "            # Unigrams from all tokens\n",
        "            for token in tokens:\n",
        "                unigrams[token] += count\n",
        "    total_unigrams = sum(unigrams.values())\n",
        "    total_bigrams = sum(bigrams.values())\n",
        "    total_trigrams = sum(trigrams.values())\n",
        "    total_fourgrams = sum(fourgrams.values())\n",
        "    total_fivegrams = sum(fivegrams.values())\n",
        "    return unigrams, bigrams, trigrams, fourgrams, fivegrams, total_unigrams, total_bigrams, total_trigrams, total_fourgrams, total_fivegrams\n",
        "\n",
        "# Build n-gram counts from 'fivegrams.txt'\n",
        "# (Make sure you have a file named 'fivegrams.txt' in the working directory)\n",
        "unigrams, bigrams, trigrams, fourgrams, fivegrams, total_unigrams, total_bigrams, total_trigrams, total_fourgrams, total_fivegrams = build_all_ngrams('fivegrams.txt')\n",
        "\n",
        "ngrams = {1: unigrams, 2: bigrams, 3: trigrams, 4: fourgrams, 5: fivegrams}\n",
        "ngrams_total = {1: total_unigrams, 2: total_bigrams, 3: total_trigrams, 4: total_fourgrams, 5: total_fivegrams}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from functools import lru_cache\n",
        "\n",
        "#############################\n",
        "# Norvig’s Baseline Corrector\n",
        "#############################\n",
        "\n",
        "def known(words_list):\n",
        "    return set(w for w in words_list if w in unigrams)\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def candidates(word):\n",
        "    \"\"\"\n",
        "    Generate candidate corrections for a word.\n",
        "    Uses one or two edit distances. Returns a set.\n",
        "    \"\"\"\n",
        "    def edits1(word):\n",
        "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "        inserts = [L + c + R for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(word):\n",
        "        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "    return known([word]) or known(edits1(word)) or known(edits2(word)) or {word}\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def norvig_word_probability(word):\n",
        "    return unigrams[word] / total_unigrams\n",
        "\n",
        "def norvig_correct_word(word):\n",
        "    lower = word.lower()\n",
        "    if lower in unigrams:\n",
        "        return word\n",
        "    return max(candidates(lower), key=lambda w: norvig_word_probability(w))\n",
        "\n",
        "def norvig_correct_sentence(sentence):\n",
        "    tokens = sentence.split()\n",
        "    corrected_tokens = [norvig_correct_word(token) for token in tokens]\n",
        "    return \" \".join(corrected_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################\n",
        "# SymSpell candidate generator\n",
        "#############################\n",
        "\n",
        "MAX_SYMSPELL_DISTANCE = 2\n",
        "\n",
        "def delete_edit(word):\n",
        "    \"\"\"\n",
        "    Generate all possible strings that result from deleting one character from word.\n",
        "    \"\"\"\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    return {L + R[1:] for L, R in splits if R}\n",
        "\n",
        "def precalculate_symspell(unigrams):\n",
        "    \"\"\"\n",
        "    Pre-calculate a mapping from deletion edits to the set of original words that produced them.\n",
        "    \"\"\"\n",
        "    symspell_mapping = {}\n",
        "    for word in unigrams:\n",
        "        edits = delete_edit(word)\n",
        "        for edit in edits:\n",
        "            if edit:\n",
        "                symspell_mapping.setdefault(edit, set()).add(word)\n",
        "    return symspell_mapping\n",
        "\n",
        "symspell_mapping = precalculate_symspell(unigrams)\n",
        "\n",
        "def edit_distance(s, t, max_distance):\n",
        "    \"\"\"\n",
        "    Compute the Levenshtein edit distance between strings s and t.\n",
        "    Early exit if the distance exceeds max_distance.\n",
        "    \"\"\"\n",
        "    if abs(len(s) - len(t)) > max_distance:\n",
        "        return max_distance + 1\n",
        "\n",
        "    previous_row = list(range(len(t) + 1))\n",
        "    for i, sc in enumerate(s, start=1):\n",
        "        current_row = [i] + [0] * len(t)\n",
        "        for j, tc in enumerate(t, start=1):\n",
        "            cost = 0 if sc == tc else 1\n",
        "            current_row[j] = min(previous_row[j] + 1,        # deletion\n",
        "                                 current_row[j - 1] + 1,       # insertion\n",
        "                                 previous_row[j - 1] + cost)   # substitution\n",
        "        if min(current_row) > max_distance:\n",
        "            return max_distance + 1\n",
        "        previous_row = current_row\n",
        "    return previous_row[len(t)]\n",
        "\n",
        "def symspell_candidates(word, max_distance=2):\n",
        "    \"\"\"\n",
        "    Generate candidate corrections for a word using the SymSpell algorithm.\n",
        "    \n",
        "    Assumes:\n",
        "      - `unigrams` is a global set of valid words.\n",
        "      - `symspell_mapping` is a global mapping from deletion edits to original words.\n",
        "    \n",
        "    Returns:\n",
        "      A dictionary mapping candidate words to their edit distance from the query word.\n",
        "    \"\"\"\n",
        "    suggestions = {}\n",
        "    \n",
        "    # If the word is an exact match, add it as a candidate with distance 0.\n",
        "    suggestions[word] = 0\n",
        "\n",
        "    visited = {word}\n",
        "    candidates = {word}\n",
        "\n",
        "    while candidates:\n",
        "        candidate = candidates.pop()\n",
        "        \n",
        "        # Check if the candidate deletion exists in our mapping.\n",
        "        if candidate in symspell_mapping:\n",
        "            for suggestion in symspell_mapping[candidate]:\n",
        "                if suggestion not in suggestions:\n",
        "                    distance = edit_distance(word, suggestion, max_distance)\n",
        "                    if distance <= max_distance:\n",
        "                        suggestions[suggestion] = distance\n",
        "        \n",
        "        # Only expand further deletions if we haven't exceeded the max_distance.\n",
        "        # The number of deletions already applied is len(word) - len(candidate)\n",
        "        if len(word) - len(candidate) < max_distance:\n",
        "            for deletion in delete_edit(candidate):\n",
        "                if deletion not in visited:\n",
        "                    visited.add(deletion)\n",
        "                    candidates.add(deletion)\n",
        "    \n",
        "    return suggestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################\n",
        "# N-gram Spell Corrector\n",
        "# With Interpolated Kneser-Ney Smoothing\n",
        "# SymSpell Candidate Generator\n",
        "# Edit Distance Error Model\n",
        "#############################\n",
        "\n",
        "ERROR_PENALTY = 0.001\n",
        "KNESER_NEY_DISCOUNT = 0.75\n",
        "EPSILON = 1e-10\n",
        "MAX_NGRAM = 5\n",
        "MIN_IMPROVEMENT_MARGIN = 0.5\n",
        "\n",
        "#############################\n",
        "# Pre-calculate context counts\n",
        "#############################\n",
        "\n",
        "def build_context_counts(ngrams):\n",
        "    \"\"\"\n",
        "    Pre-calculate context counts for n-grams.\n",
        "    \n",
        "    Returns two dictionaries:\n",
        "      - context_counts: mapping from a context (tuple) to the total frequency with which it appears.\n",
        "      - context_unique_counts: mapping from a context (tuple) to the number of unique words following it.\n",
        "    \"\"\"\n",
        "    context_counts = {}  # Total count of contexts\n",
        "    context_unique_counts = {}  # Number of unique words that follow each context\n",
        "    \n",
        "    # For n-grams where n >= 2, the context is the first n-1 tokens.\n",
        "    for n in range(2, MAX_NGRAM + 1):\n",
        "        context_counts[n - 1] = collections.Counter()\n",
        "        context_unique_counts[n - 1] = collections.Counter()\n",
        "        for ngram, count in ngrams.get(n, {}).items():\n",
        "            context = ngram[:-1]\n",
        "            context_counts[n - 1][context] += count\n",
        "            context_unique_counts[n - 1][context] += 1\n",
        "    return context_counts, context_unique_counts\n",
        "\n",
        "context_counts, context_unique_counts = build_context_counts(ngrams)\n",
        "\n",
        "#############################\n",
        "# Interpolated Kneser-Ney Smoothing\n",
        "#############################\n",
        "@lru_cache(maxsize=None)\n",
        "def calculate_backoff(context):\n",
        "    \"\"\"\n",
        "    Calculate the backoff weight for a given context.\n",
        "    \"\"\"\n",
        "    context_len = len(context)\n",
        "    if context_len == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Get the frequency of the context. If absent, assume 0.\n",
        "    context_freq = context_counts.get(context_len, {}).get(context, 0)\n",
        "    if context_freq == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Get the number of unique continuations.\n",
        "    unique_count = context_unique_counts.get(context_len, {}).get(context, 0)\n",
        "    return KNESER_NEY_DISCOUNT * unique_count / context_freq\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def kneser_ney_language_model(word, context):\n",
        "    \"\"\"\n",
        "    Calculate the interpolated Kneser-Ney probability of a word given a context.\n",
        "    \n",
        "    Args:\n",
        "        word: The target word (string).\n",
        "        context: A tuple of preceding words.\n",
        "    \n",
        "    Returns:\n",
        "        The probability of 'word' given 'context' under the interpolated Kneser-Ney model.\n",
        "    \"\"\"\n",
        "    context_len = len(context)\n",
        "    \n",
        "    # Base case: empty context; use the unigram probability.\n",
        "    if context_len == 0:\n",
        "        return norvig_word_probability(word)\n",
        "    \n",
        "    # If context is longer than allowed, use the last (MAX_NGRAM-1) words.\n",
        "    if context_len >= MAX_NGRAM:\n",
        "        context = context[-(MAX_NGRAM - 1):]\n",
        "        context_len = MAX_NGRAM - 1\n",
        "\n",
        "    ngram_count = ngrams.get(context_len + 1, {}).get(context + (word,), 0)\n",
        "    context_total = context_counts.get(context_len, {}).get(context, 0)\n",
        "\n",
        "    \n",
        "    if context_total == 0:\n",
        "        return kneser_ney_language_model(word, context[1:])\n",
        "    else:\n",
        "        discounted_prob = max(ngram_count - KNESER_NEY_DISCOUNT, 0) / context_total\n",
        "\n",
        "    backoff = calculate_backoff(context)\n",
        "    \n",
        "    # Recursive backoff to a lower-order model by removing the first word of the context.\n",
        "    return discounted_prob + backoff * kneser_ney_language_model(word, context[1:])\n",
        "\n",
        "\n",
        "#############################\n",
        "# Error Model\n",
        "#############################\n",
        "\n",
        "def error_model(candidate_distance):\n",
        "    \"\"\"\n",
        "    Calculate the probability of an error given the edit distance.\n",
        "    \"\"\"\n",
        "    return ERROR_PENALTY ** candidate_distance\n",
        "\n",
        "\n",
        "#############################\n",
        "# N-gram Spell Corrector\n",
        "#############################\n",
        "\n",
        "def ngram_correct_word(word, context):\n",
        "    \"\"\"\n",
        "    Correct a word using the N-gram model with interpolated Kneser-Ney smoothing.\n",
        "    \"\"\"\n",
        "    if word in unigrams:\n",
        "        return word\n",
        "    # Compute score for the original word\n",
        "    orig_lm_prob = kneser_ney_language_model(word, context)\n",
        "    orig_score = orig_lm_prob * error_model(0)\n",
        "    orig_log = math.log(orig_score + EPSILON)\n",
        "\n",
        "    # Generate candidate corrections using SymSpell.\n",
        "    candidates_dict = symspell_candidates(word, MAX_SYMSPELL_DISTANCE)\n",
        "    candidate_scores = {}\n",
        "    for candidate, distance in candidates_dict.items():\n",
        "        lm_prob = kneser_ney_language_model(candidate, context)\n",
        "        candidate_prob = lm_prob * error_model(distance)\n",
        "        candidate_scores[candidate] = candidate_prob\n",
        "\n",
        "    # Find the candidate with the highest probability\n",
        "    best_candidate = max(candidate_scores, key=candidate_scores.get)\n",
        "    best_log = math.log(candidate_scores[best_candidate] + EPSILON)\n",
        "\n",
        "    # Only correct if the candidate is different and its score is better by the threshold margin.\n",
        "    if best_candidate != word and (best_log - orig_log) > MIN_IMPROVEMENT_MARGIN:\n",
        "        return best_candidate\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def ngram_correct_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Correct a sentence using the N-gram model with interpolated Kneser-Ney smoothing.\n",
        "    \"\"\"\n",
        "    tokens = sentence.lower().split()\n",
        "    corrected_tokens = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        context = tuple(tokens[max(0, i - MAX_NGRAM + 1):i])\n",
        "        corrected_tokens.append(ngram_correct_word(token, context))\n",
        "    return \" \".join(corrected_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################\n",
        "# Beam Search\n",
        "#############################\n",
        "\n",
        "BEAM_WIDTH = 10\n",
        "LOOKAHEAD_DEPTH  = 2\n",
        "\n",
        "def lookahead_score(tokens, index, context, depth):\n",
        "    \"\"\"\n",
        "    Recursively estimate an optimistic additional log score for the next 'depth' tokens.\n",
        "    \n",
        "    For each token in the lookahead window, if the token exists in our vocabulary,\n",
        "    we assume the best candidate is the token itself.\n",
        "    \"\"\"\n",
        "    if index >= len(tokens) or depth == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    token = tokens[index]\n",
        "    lm_prob = kneser_ney_language_model(token, context)\n",
        "    log_prob = math.log(lm_prob + EPSILON)\n",
        "    new_context = (context + (token,))[-(MAX_NGRAM - 1):]\n",
        "    return log_prob + lookahead_score(tokens, index + 1, new_context, depth - 1)\n",
        "\n",
        "\n",
        "def beam_search_sentence(sentence, beam_width=BEAM_WIDTH, lookahead_depth=LOOKAHEAD_DEPTH):\n",
        "    \"\"\"\n",
        "    Correct a sentence using beam search with lookahead.\n",
        "    \n",
        "    At each token position, we generate candidate corrections (using symspell_candidates)\n",
        "    and score them using our Kneser-Ney language model combined with the error model.\n",
        "    A lookahead heuristic estimates the future score for remaining tokens.\n",
        "    \n",
        "    Returns:\n",
        "      The corrected sentence (as a string).\n",
        "    \"\"\"\n",
        "    tokens = sentence.lower().split()\n",
        "    beam = [([], 0.0, tuple())]\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        new_beam = []\n",
        "        for corrected_tokens, cum_log_score, context in beam:\n",
        "            # Compute the original token's log score in this context.\n",
        "            orig_lm_prob = kneser_ney_language_model(token, context)\n",
        "            orig_score = orig_lm_prob * error_model(0)\n",
        "            orig_log = math.log(orig_score + EPSILON)\n",
        "\n",
        "            # Generate candidate corrections.\n",
        "            candidates = symspell_candidates(token, MAX_SYMSPELL_DISTANCE)\n",
        "\n",
        "            for candidate, distance in candidates.items():\n",
        "                # Compute candidate score.\n",
        "                lm_prob = kneser_ney_language_model(candidate, context)\n",
        "                candidate_prob = lm_prob * error_model(distance)\n",
        "                log_candidate_prob = math.log(candidate_prob + EPSILON)\n",
        "\n",
        "                # If the candidate is not the same as the original,\n",
        "                # only allow it if its log score improves by the threshold margin.\n",
        "                if candidate != token and (log_candidate_prob - orig_log) < MIN_IMPROVEMENT_MARGIN:\n",
        "                    continue  # skip candidate\n",
        "\n",
        "                new_cum_log = cum_log_score + log_candidate_prob\n",
        "                new_context = (context + (candidate,))[-(MAX_NGRAM - 1):]\n",
        "\n",
        "                heuristic = lookahead_score(tokens, i + 1, new_context, lookahead_depth)\n",
        "                total_estimated_score = new_cum_log + heuristic\n",
        "\n",
        "                new_beam.append((corrected_tokens + [candidate], new_cum_log, new_context, total_estimated_score))\n",
        "\n",
        "        new_beam.sort(key=lambda x: x[3], reverse=True)\n",
        "        beam = [(ct, cl, ctx) for ct, cl, ctx, _ in new_beam[:beam_width]]\n",
        "\n",
        "    best_candidate = max(beam, key=lambda state: state[1])\n",
        "    return \" \".join(best_candidate[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "## Data:\n",
        "---\n",
        "We extact the ngrams from the provided \"fivegrams.txt\" dataset. Our function build_all_ngrams reads a file where each line includes a count and five tokens. We compute counts for unigrams through fivegrams and store totals. The resulting ngrams dictionary and total counts are later used by the language models.\n",
        "\n",
        "## Spell Correction Algorithms:\n",
        "---\n",
        "### Norvig’s Baseline Corrector:\n",
        "A well‐known method that uses one- and two-edit transformations to generate candidate corrections and then chooses the candidate with the highest unigram probability from a large corpus.\n",
        "\n",
        "### N-gram Model with Interpolated Kneser–Ney Smoothing:\n",
        "This approach leverages higher-order n‑gram statistics (up to five-grams) to compute the probability of a candidate word given its context. Kneser–Ney smoothing is applied to better handle unseen n‑grams, and a candidate’s likelihood is combined with an error model (based on edit distance) to penalize corrections that require many changes.\n",
        "\n",
        "### Beam Search with Lookahead:\n",
        "Building on the n‑gram model, this method performs a global search over the entire sentence. At each token, candidate corrections are scored, and the algorithm maintains a beam of top hypotheses. A lookahead heuristic (which also uses Kneser–Ney probabilities) is optionally incorporated to provide an estimate of future score, aiming to balance immediate corrections with longer-range context.\n",
        "\n",
        "## Implementation improvements:\n",
        "---\n",
        "### SymSpell\n",
        "A faster candidate generation algorithm, uses only 1 edit operation and precomputed neighbouring words.\n",
        "\n",
        "### Interpolated Kneser-Ney smoothing\n",
        "The core of our algorithms is the kneser_ney_language_model, which calculates the probability of a word given its context. It uses recursive backoff and discounting based on pre-calculated context counts.\n",
        "\n",
        "### Error Model\n",
        "We incorporate an error penalty based on edit distance (implemented as error_model). This model penalizes corrections that require more changes.\n",
        "\n",
        "### Thresholding\n",
        "A minimum improvement margin is used to decide whether to change a word. If a candidate’s (log) score isn’t better than that of the original word by a set threshold, the original is kept.\n",
        "\n",
        "### Beam Search\n",
        "A beam of top hypotheses is maintained to control search complexity. An optional lookahead function estimates the best future score by assuming the best candidate for upcoming tokens is the token itself (using Kneser–Ney probabilities). This value is weighted before being added to the cumulative score.\n",
        "\n",
        "## Obstacles:\n",
        "---\n",
        "Although both n-gram approaches showed better in-context accuracy for spell correction, they greatly suffered from over-correction. To prevent our algorithms from replacing correct words we had to introduce distance based penalties and improvement thresholding.\n",
        "\n",
        "## Evaluation:\n",
        "---\n",
        "### Dataset:\n",
        "We evaluated our models on the Holbrook dataset, which provides sentences with error markup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluation on Holbrook Dataset ===\n",
            "\n",
            "norvig_correct_sentence: (0.7184713911500502, 0.15640287737394507, 0.18021109371262453, 0.13899407014260975, 373, 4840, 1340)\n",
            "ngram_correct_sentence: (0.7440060209447952, 0.14394864457467987, 0.19674377155650577, 0.14043256551595992, 332, 4881, 818)\n",
            "beam_search_sentence: (0.7433419808360842, 0.17310096989954568, 0.21874368349087447, 0.16543528273444696, 390, 4823, 904)\n"
          ]
        }
      ],
      "source": [
        "def parse_holbrook_sentence(line):\n",
        "    \"\"\"\n",
        "    Given a line from the Holbrook dataset, return a tuple (erroneous, correct).\n",
        "    We remove the error tags for the erroneous version, keeping the inner (erroneous) text.\n",
        "    For the correct version, we replace the entire <ERR targ=...>...</ERR> with the value of targ.\n",
        "    \n",
        "    For example:\n",
        "      \"I have four ... and <ERR targ=sister> siter </ERR> .\"\n",
        "    returns:\n",
        "      erroneous: \"I have four ... and siter .\"\n",
        "      correct:   \"I have four ... and sister .\"\n",
        "    \"\"\"\n",
        "    # Pattern to capture error tags: <ERR targ=XXX> YYY </ERR>\n",
        "    pattern = re.compile(r'<ERR\\s+targ=([^>]+)>\\s*([^<]+)\\s*</ERR>')\n",
        "    \n",
        "    # Create erroneous version: simply remove the tag markup.\n",
        "    erroneous = pattern.sub(r'\\2', line)\n",
        "    # Create corrected version: replace entire tag with the target attribute.\n",
        "    correct = pattern.sub(r'\\1', line)\n",
        "    \n",
        "    # Remove any extra whitespace.\n",
        "    erroneous = re.sub(r'\\s+', ' ', erroneous).strip().lower()\n",
        "    correct = re.sub(r'\\s+', ' ', correct).strip().lower()\n",
        "    return erroneous, correct\n",
        "\n",
        "def evaluate_holbrook(corpus_file, correction_functions, num_samples=None):\n",
        "    \"\"\"\n",
        "    Evaluate the correction functions using the Holbrook dataset.\n",
        "    For each sentence, compute token-level accuracy: (# tokens matching gold) / (total gold tokens).\n",
        "    Returns the average accuracy per correction function.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    with open(corpus_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                erroneous, correct = parse_holbrook_sentence(line)\n",
        "                # Only consider sentences where corrections are present.\n",
        "                if erroneous != correct:\n",
        "                    sentences.append((erroneous, correct))\n",
        "            if num_samples and len(sentences) >= num_samples:\n",
        "                break\n",
        "    \n",
        "    metrics = {}  # Maps correction function name to a list of per-sentence metrics.\n",
        "    for erroneous, gold in sentences:\n",
        "        gold_tokens = gold.split()\n",
        "        erroneous_tokens = erroneous.split()\n",
        "        for func in correction_functions:\n",
        "            corrected = func(erroneous)\n",
        "            corr_tokens = corrected.split()\n",
        "            # Align tokens using zip (assuming tokenization is consistent).\n",
        "            true_positives = sum(1 for g, er, c in zip(gold_tokens, erroneous_tokens, corr_tokens) if g == c and g != er)\n",
        "            false_negatives = sum(1 for g, er, c in zip(gold_tokens, erroneous_tokens, corr_tokens) if g != c and g == er)\n",
        "            false_positives = sum(1 for g, er, c in zip(gold_tokens, erroneous_tokens, corr_tokens) if g != c and g != er)\n",
        "            \n",
        "            # Accuracy: fraction of tokens in the corrected sentence that are correct.\n",
        "            accuracy = sum(1 for g, c in zip(gold_tokens, corr_tokens) if g == c) / len(gold_tokens)\n",
        "            precision = (true_positives / (true_positives + false_positives)) if (true_positives + false_positives) > 0 else 0.0\n",
        "            recall = (true_positives / (true_positives + false_negatives)) if (true_positives + false_negatives) > 0 else 0.0\n",
        "            f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "            # Store per-sentence metrics.\n",
        "            metrics.setdefault(func.__name__, []).append(\n",
        "                (accuracy, precision, recall, f1, true_positives, false_positives, false_negatives)\n",
        "            )\n",
        "    \n",
        "    # Average metrics over all sentences.\n",
        "    avg_metrics = {}\n",
        "    for func, scores in metrics.items():\n",
        "        avg_accuracy = sum(score[0] for score in scores) / len(scores)\n",
        "        avg_precision = sum(score[1] for score in scores) / len(scores)\n",
        "        avg_recall = sum(score[2] for score in scores) / len(scores)\n",
        "        avg_f1 = sum(score[3] for score in scores) / len(scores)\n",
        "        total_tp = sum(score[4] for score in scores)\n",
        "        total_fp = sum(score[5] for score in scores)\n",
        "        total_fn = sum(score[6] for score in scores)\n",
        "        avg_metrics[func] = (avg_accuracy, avg_precision, avg_recall, avg_f1,\n",
        "                             total_tp, total_fp, total_fn)\n",
        "    \n",
        "    return avg_metrics\n",
        "\n",
        "print(\"\\n=== Evaluation on Holbrook Dataset ===\\n\")\n",
        "holbrook_metrics = evaluate_holbrook('holbrook-tagged.txt', \n",
        "                                        correction_functions=[\n",
        "                                            norvig_correct_sentence, \n",
        "                                            ngram_correct_sentence,\n",
        "                                            beam_search_sentence\n",
        "                                            ],\n",
        "                                        # num_samples=1000\n",
        "                                        )\n",
        "for func, score in holbrook_metrics.items():\n",
        "    print(f\"{func}: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
